<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Holistic Generation of Slide Decks</title>

  <!-- Reveal.js styles -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css" id="theme">

  <!-- Custom overrides -->
  <style>
    .reveal .slides section {
      /* Ensure slides take full height/width for background images */
      height: 100% !important;
    }

  </style>
</head>

<body>
  <div class="reveal">
    <div class="slides">
      <!-- Slides -->
      <section id="slide01" data-background="generated_slides/slide_01_0.jpg" data-background-size="contain">
        <aside class="notes">
          Good morning everyone. Before we begin, look at the slide behind me. That glass banana... it's not a 3D render
          I spent days building in Blender. It's not a photograph. It's fully generated by AI.
          Today, I want to talk about a radical shift in how we create presentations. For decades, we've been stuck in
          the same workflow: finding templates, dragging boxes, and forcing disparate elements to look good together.
          But what if we could stop 'assembling' slides and start 'rendering' them?
          I'm going to introduce you to a concept I call the 'Generative Kernel'. This is a new paradigm where we use
          Nano Banana Pro not just to make stickers or background images, but to generate entire, holistic slide decks
          that rival the visual fidelity of a Steve Jobs keynote.
          We're going to explore how we can move from the tedious collage work of the past to a future where high-end
          design is computationally generated. Let's dive in.
        </aside>
      </section>

      <section id="slide02" data-background="generated_slides/slide_02_0.jpg" data-background-size="contain">
        <aside class="notes">
          Let's frame the problem. When most of us make slides, we are fighting a losing battle against fragmentation.
          You find an icon from one pack, a stock photo from another, and a font that doesn't quite match either. The
          result is what you see on the left: text that fights for attention and visuals that feel disjointed.
          But look at the right side of this slide. That transformation from chaotic plastic bricks into a perfect
          liquid metal sphere... that is the shift we are aiming for. We want to move from 'assembling'—where you pile
          independent blocks on top of each other—to 'rendering'.
          In a rendering workflow, the entire slide is calculated as a single, cohesive scene. Light, shadow, material,
          and perspective are unified. It’s the difference between building a Lego house and pouring a continuous
          concrete sculpture. We aren't just making slides faster; we are fundamentally changing the nature of the
          output to achieve a level of visual integrity that was previously impossible for non-designers.
        </aside>
      </section>

      <section id="slide03" data-background="generated_slides/slide_03_0.jpg" data-background-size="contain">
        <aside class="notes">
          To really appreciate this, let's look at where we are coming from. This is the 'Old Way'. Does this look
          familiar? It’s the classic 'Bad PPT' aesthetic.
          You have a stark white background. You have a generic font like Arial. And then you have those icons... a flat
          gear, a generic stick figure, a computer. They don't share a line weight, they don't share a perspective, and
          they certainly don't look like they live in the same universe.
          This is what happens when you build 'bottom-up'. You are hunting for assets one by one. You spend hours
          manually aligning boxes, tweaking font sizes, and trying to put lipstick on a pig. But no matter how much you
          align them, they never truly cohere. This workflow forces us to be layout operators rather than storytellers.
          It consumes our time and gives us mediocrity in return.
        </aside>
      </section>

      <section id="slide04" data-background="generated_slides/slide_04_0.jpg" data-background-size="contain">
        <aside class="notes">
          Now, look at the 'New Way'. This is the same content—Process, People, Technology—but reimagined through a
          Generative Kernel.
          Notice how the concepts aren't isolated icons anymore. They are organic glass capsules, connected by living
          strands of light. They feel like part of a single ecosystem. The light that hits the 'Process' capsule
          refracts and illuminates the 'People' capsule. They are physically aware of each other.
          This is 'Holistic Scene Generation'. We didn't place these objects manually x-pixels apart. We described a
          scene, and the AI rendered the relationships. The visual integrity is perfect because it's calculated, not
          assembled. This visual consistency is what gives high-end presentations their power—subconsciously telling the
          audience that your ideas are coherent, connected, and polished.
        </aside>
      </section>

      <section id="slide05" data-background="generated_slides/slide_05_0.jpg" data-background-size="contain">
        <aside class="notes">
          However, I have to be honest with you. If you just go to Nano Banana Pro right now and ask for 'a presentation
          about AI', you will fail.
          I tried this. And I hit three massive walls, which I call the Reality Check.
          First, 'Entropy'. AI has no memory of the previous slide. Slide 1 might be glass, Slide 2 might be cartoon,
          Slide 3 might be photorealistic. The styles drift uncontrollably.
          Second, 'Hallucination'. You ask for a QR code, it paints a maze that doesn't scan. You ask for text, it gives
          you alien hieroglyphs. It creates beautiful nonsense.
          Third, 'Isolation'. You get a flat JPG. You can't click a link. You can't select text. You can't play a video.
          It's a dead image.
          So, while the visual promise is incredible, the practical reality is broken. We need a way to harness the
          power while taming the chaos.
        </aside>
      </section>

      <section id="slide06" data-background="generated_slides/slide_06_0_4k.jpg" data-background-size="contain">
        <aside class="notes">
          This brings us to the solution: The Generative Kernel.
          Think of it like this slide shows: It's a deterministic container for probabilistic creativity.
          At the center, we have that swirling, colorful nebula. That is the AI—Nano Banana Pro. It’s creative, wild,
          and unpredictable. It provides the magic.
          But if we just let it spill out, it makes a mess. So we encase it in a rigid, structural sphere—our
          'Container'.
          Visualized here, Layer 1 is 'Deterministic Code'—things like HTML/CSS that never fail. Layer 2 is the
          'Probabilistic AI' that generates the pixels. And Layer 3 consists of our 'Constraints & SOPs'—the rules we
          give the AI to keep it in check.
          By building this architecture, we get the best of both worlds: the infinite creativity of AI, held within the
          reliable bounds of software engineering.
        </aside>
      </section>

      <section id="slide07" data-background="generated_slides/slide_07_0.jpg" data-background-size="contain">
        <aside class="notes">
          Let's break down the three techniques that make this work. Technique number one is 'Visual Anchoring'.
          We all know that describing a visual style in words is hard. 'Make it professional' means different things to
          everyone. So instead of describing, we show.
          On the right, you see our 'Style Matrix'. This is a reference image we feed into every single generation call.
          It acts as a visual anchor.
          We tell the AI: 'Don't imagine a style. Look at this image, and renders the new content using *these*
          materials, *this* lighting, and *this* vibe.'
          As you can see in the transformation here, we take a simple input—a wireframe idea—and pass it through the
          lens of the Style Matrix. The result is that every slide, no matter the content, feels like it was shot by the
          same photographer in the same studio.
        </aside>
      </section>

      <section id="slide08" data-background="generated_slides/slide_08_0.jpg" data-background-size="contain">
        <aside class="notes">
          Technique number two is 'Asset Injection'. This solves the hallucination problem.
          AI is terrible at drawing precise things like logos or QR codes. So we don't let it draw them.
          Instead, we take the raw pixel assets—like the Superlinear logo or this QR code—and we 'inject' them into the
          prompt as image inputs.
          But here's the magic: We don't just paste them on top. We ask the AI to 'embed' them into the scene. Look at
          that QR code on the ceramic surface. It's not a sticker. It has texture, it has shadow, it looks etched into
          the material. The AI handles the lighting integration, but the data fidelity remains perfect. It scans. It
          works.
          This is Context Curation: We provide the truth (the pixels), and the AI provides the beauty (the integration).
        </aside>
      </section>

      <section id="slide09" data-background="generated_slides/slide_09_0.jpg" data-background-size="contain">
        <aside class="notes">
          The third technique is 'Delayed Rendering'.
          Generating these images is expensive and slow. If you re-render every time you fix a typo, you'll go broke and
          crazy.
          So we borrowed a concept from software engineering. We separate 'Source' from 'Build'.
          We write everything in Markdown text first—our 'Content Logic'. We iterate on the story, the flow, the words.
          It's fast and cheap.
          Only when we 'Lock-down' the content—that padlock you see—do we send it to the 'Batch Render' engine.
          This engine creates the slides in one go. It’s an optimization that allows us to focus on the narrative
          structure without being distracted by visual tweaks until the very end. It turns presentation design into a
          compile process.
        </aside>
      </section>

      <section id="slide10" data-background="generated_slides/slide_10_0.jpg" data-background-size="contain">
        <aside class="notes">
          So, what does this kit actually look like?
          It’s not a single app. It’s what I call a methodology for 'User Generated Software'.
          On this workbench, you see the three parts we are releasing.
          First, the 'Core Kit'—the code skeleton. This is the Python and JavaScript that runs the show.
          Second, the 'Guiding Knowledge'—that glowing manual. This is the system instructions we give to the AI so it
          knows how to be a presentation designer.
          And third, the 'Leverage Tools'—scripts and pipelines that handle the assets and API calls.
          We aren't just giving you a fish; we are giving you a robotic fishing pole. This kit allows you to build your
          own presentation engines, tailored to your own style.
        </aside>
      </section>

      <section id="slide11" data-background="generated_slides/slide_11_0.jpg" data-background-size="contain">
        <aside class="notes">
          I want to end on a philosophical note.
          In software, we have a principle called DRY: Don't Repeat Yourself. We build efficient, standardized products
          like that grey chair on the left. It works, but it's static. It's boring.
          But in this new era of AI, we are moving 'Beyond DRY'. We are moving towards 'Generative Potential'.
          Look at the structure on the right. It's self-assembling, dynamic, and unique.
          When we deliver a system like this Generative Kernel, we are optimizing for 'Intention Fidelity'—how close can
          we get to what's in your head? And 'Expression Range'—how many differenet things can you create?
          We are no longer just delivering a static deck. We are delivering the potential to generate infinite decks.
        </aside>
      </section>
    </div>
  </div>

  <!-- Load Reveal.js -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/notes/notes.js"></script>

  <script>
    // Initialize Reveal.js
    Reveal.initialize({
      width: 1920,
      height: 1080,
      hash: true,
      center: true,
      history: true,
      transition: 'fade', // cinematic transition
      backgroundTransition: 'fade',
      controls: false, // minimalist
      progress: true,
      minScale: 0.2,
      maxScale: 2.0,
      plugins: [RevealNotes]
    });
  </script>
</body>

</html>